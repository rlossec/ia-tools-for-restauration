{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d188c-0a07-4495-886d-7acb43368b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cfa441-420d-4eb5-a108-6294e21726d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9be2b3-f1c6-40e7-acf1-011c845a1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410727c2-f3e1-4b70-a3d4-14652bb69da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/2_preprocessed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a20ce6-d2a4-4661-ab04-5773707963d5",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4f3bee-51de-4ff7-952b-79a177040694",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews = pd.read_csv(DATA_PATH + 'bad_reviews',\n",
    "                          dtype={\n",
    "                                 \"review_id\": str,\n",
    "                                 \"user_id\": str,\n",
    "                                 \"business_id\": str,\n",
    "                                 \"stars\": 'uint8',\n",
    "                                 \"useful\": 'int16',\n",
    "                                 \"funny\": 'int16',\n",
    "                                 \"cool\": 'int16',\n",
    "                                 \"text\": str,\n",
    "                                } \n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622b8fc2-6453-4bc3-9c91-82336c98ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = pd.read_csv(DATA_PATH + 'good_reviews',\n",
    "                          dtype={\n",
    "                                 \"review_id\": str,\n",
    "                                 \"user_id\": str,\n",
    "                                 \"business_id\": str,\n",
    "                                 \"stars\": 'uint8',\n",
    "                                 \"useful\": 'int16',\n",
    "                                 \"funny\": 'int16',\n",
    "                                 \"cool\": 'int16',\n",
    "                                 \"text\": str,\n",
    "                                } \n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3d0e12-ec0f-4467-8605-cabdc17864b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = pd.read_csv(DATA_PATH + 'sample_reviews',\n",
    "                          dtype={\n",
    "                                 \"review_id\": str,\n",
    "                                 \"user_id\": str,\n",
    "                                 \"business_id\": str,\n",
    "                                 \"stars\": 'uint8',\n",
    "                                 \"useful\": 'int16',\n",
    "                                 \"funny\": 'int16',\n",
    "                                 \"cool\": 'int16',\n",
    "                                 \"text\": str,\n",
    "                                } \n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97cfd4f-04ee-491c-a46d-3d3d79cd8d42",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbb7662-a037-4894-b5b2-7efa16f01190",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bad    = \". \".join(list(bad_reviews.text))\n",
    "corpus_good   = \". \".join(list(good_reviews.text))\n",
    "corpus_sample = \". \".join(list(sample_reviews.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fcb2e0-63fc-4120-a71b-31610066207f",
   "metadata": {},
   "source": [
    "## Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1588ed2-c3e6-4904-887d-a796231749e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(doc,\n",
    "                 stop_words,\n",
    "                 rejoin=False,\n",
    "                 lem_or_stem=\"stem\",\n",
    "                 rare_words=None,\n",
    "                 min_len_word=3,\n",
    "                 force_is_alpha=True,\n",
    "                 context=None,\n",
    "                 english_words=None):\n",
    "    \"\"\"Process a text with selection of english words\n",
    "    positionnal args :\n",
    "    -------------------\n",
    "    doc: str: the document to process\n",
    "    \n",
    "    optional args :\n",
    "    ----------------\n",
    "    rejoin: bool: if False return a list of words, else return the string of joined element of the list\n",
    "    lem_or_stem : choice between lemmatize and stemmatize\n",
    "    rare_words: list : list of rare words to exclude\n",
    "    force_is_alpha: int: if 1 exclude all tokens with a numeric character\n",
    "    english_words: list: list of english words accepted\n",
    "    \n",
    "    return:\n",
    "    --------\n",
    "    List of tokens if rejoin=False, the joined list if True.\n",
    "    \"\"\"\n",
    "    if not rare_words:\n",
    "        rare_words=[]\n",
    "    \n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    cleaned_tokens_list = [word for word in raw_tokens_list if word not in stop_words]\n",
    "    \n",
    "    non_rare_tokens = [word for word in cleaned_tokens_list if word not in rare_words]\n",
    "    \n",
    "    large_words = [word for word in non_rare_tokens if len(word) >= min_len_word]\n",
    "    \n",
    "    if force_is_alpha:\n",
    "        alpha_tokens = [word for word in large_words if word.isalpha()]\n",
    "    else:\n",
    "        alpha_tokens = large_words\n",
    "        \n",
    "    if lem_or_stem == \"lem\":\n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens]\n",
    "    else:\n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens]\n",
    "        \n",
    "    if context:\n",
    "        new_text = [word for word in trans_text if word not in context]\n",
    "    else:\n",
    "        new_text = trans_text\n",
    "        \n",
    "    if english_words:\n",
    "        english_text = [i for i in new_text if i in english_words]\n",
    "    else:\n",
    "        english_text = new_text\n",
    "        \n",
    "    if rejoin:\n",
    "        return \" \".join(english_text)\n",
    "    else:\n",
    "        return english_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b112ea-4d7e-4313-a0d6-8604b285ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_info(tokens):\n",
    "    \"\"\"Display info about corpus\"\"\"\n",
    "    print(f\"Nb tokens : {len(tokens)}\")\n",
    "    print(f\"Nb tokens uniques : {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e727a-83a9-4a58-bc36-0acd4a4b7865",
   "metadata": {},
   "source": [
    "## Process corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a0fc1d-4fdd-4493-8b87-7208bb827c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6719e724-9ea9-4aa7-a117-234e1bfd6ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_bad_tokens = preprocess_text(corpus_bad, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01a0c741-e2b2-4297-a376-79c9342b2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb tokens : 368787\n",
      "Nb tokens uniques : 11543\n",
      "['late', 'post', 'loufest', 'group', 'decid', 'nourish', 'delici', 'greasi', 'food', 'bed', 'courtesi', 'diner', 'next', 'door', 'pack', 'opt', 'steak', 'shake', 'amaz', 'cheap', 'serious', 'everyth', 'like', 'littl', 'nicer', 'mcdonald', 'imo', 'got', 'frisco', 'melt']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_info(corpus_bad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7b2b08b-18a1-4ed1-b933-439606f5a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_good_tokens = preprocess_text(corpus_good, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9af9fa9-2321-48e7-bd40-e00d809befcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb tokens : 299513\n",
      "Nb tokens uniques : 11642\n",
      "['mickey', 'sweetheart', 'fun', 'spirit', 'sandwich', 'best', 'huge', 'sandwhich', 'critic', 'got', 'italian', 'wait', 'tri', 'london', 'broil', 'next', 'best', 'vegan', 'place', 'area', 'alway', 'come', 'new', 'stuff', 'keep', 'around', 'classic', 'know', 'anyth', 'tri']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_info(corpus_good_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cce518c-4ac2-44f4-8806-3fa05d137af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_tokens = preprocess_text(corpus_sample, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db9dc2d2-319d-4fb0-a62e-c34d92c5a993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb tokens : 1072372\n",
      "Nb tokens uniques : 21825\n",
      "['late', 'post', 'loufest', 'group', 'decid', 'nourish', 'delici', 'greasi', 'food', 'bed', 'courtesi', 'diner', 'next', 'door', 'pack', 'opt', 'steak', 'shake', 'amaz', 'cheap', 'serious', 'everyth', 'like', 'littl', 'nicer', 'mcdonald', 'imo', 'got', 'frisco', 'melt']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_info(corpus_sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e0214-79d7-4472-a4ad-8fc12770a071",
   "metadata": {},
   "source": [
    "## Rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "474755b9-1dca-4a8c-ab9b-07d31c852b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food           16511\n",
       "order          12392\n",
       "place          11935\n",
       "good           11897\n",
       "time            8899\n",
       "               ...  \n",
       "perfecta           1\n",
       "combinación        1\n",
       "panni              1\n",
       "willamett          1\n",
       "flint              1\n",
       "Length: 21825, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.Series(corpus_sample_tokens).value_counts()\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "739ddb82-cbd4-44d3-ae89-03d69845be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = word_counts[word_counts==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3650c02b-47c0-4da4-a5e7-d832b5b72fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "861430f8-6e27-4459-8386-1f0586ecf3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(unique_words.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51c8e1db-84ef-4890-b664-2df9df38e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_5_words = list(word_counts[word_counts<5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d96efc-70fd-41de-853e-27d015b9c2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14494"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(less_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "482dbb5b-ce89-48cd-a04e-6a2d9277f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_10_words = list(word_counts[word_counts<10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1a8f4e-919b-47e3-ae50-ffd9f8a92ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16789"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(less_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0334005-23e4-4a11-bb32-845448eca74a",
   "metadata": {},
   "source": [
    "## Without Rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b686e8d3-a9cd-4a56-a8ca-2042034c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bad_tokens_2 = preprocess_text(corpus_bad,\n",
    "                            stop_words=stop_words,\n",
    "                            rare_words=less_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f4ceb29-5b0b-47d1-96f5-e3f7a1a1894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_good_tokens_2 = preprocess_text(corpus_good,\n",
    "                             stop_words=stop_words,\n",
    "                             rare_words=less_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac820787-5088-43ab-be7b-9f3d5ab6b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_tokens_2 = preprocess_text(corpus_sample,\n",
    "                               stop_words=stop_words,\n",
    "                               rare_words=less_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6be807b5-0e20-4120-a233-3cd8954ed586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb tokens : 362250\n",
      "Nb tokens uniques : 8902\n",
      "['late', 'post', 'group', 'decid', 'nourish', 'delici', 'greasi', 'food', 'bed', 'courtesi', 'diner', 'next', 'door', 'pack', 'opt', 'steak', 'shake', 'amaz', 'cheap', 'serious', 'everyth', 'like', 'littl', 'nicer', 'mcdonald', 'imo', 'got', 'frisco', 'melt', 'cheap']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_info(corpus_bad_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19591e0f-b50d-49e0-8d5e-dec0f89cd503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb tokens : 289429\n",
      "Nb tokens uniques : 8485\n",
      "['mickey', 'sweetheart', 'fun', 'spirit', 'sandwich', 'best', 'huge', 'sandwhich', 'critic', 'got', 'italian', 'wait', 'tri', 'london', 'broil', 'next', 'best', 'vegan', 'place', 'area', 'alway', 'come', 'new', 'stuff', 'keep', 'around', 'classic', 'know', 'anyth', 'tri']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_info(corpus_good_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0697461a-59f5-4968-8a90-404b4a4412b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb tokens : 1043943\n",
      "Nb tokens uniques : 13735\n",
      "['late', 'post', 'group', 'decid', 'nourish', 'delici', 'greasi', 'food', 'bed', 'courtesi', 'diner', 'next', 'door', 'pack', 'opt', 'steak', 'shake', 'amaz', 'cheap', 'serious', 'everyth', 'like', 'littl', 'nicer', 'mcdonald', 'imo', 'got', 'frisco', 'melt', 'cheap']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_info(corpus_sample_tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e99617-87c1-4893-ad8b-b4be48733108",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supprimer le contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07e37228-fc48-4f8a-882a-bcde151101c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order',\n",
       " 'food',\n",
       " 'time',\n",
       " 'get',\n",
       " 'place',\n",
       " 'one',\n",
       " 'like',\n",
       " 'servic',\n",
       " 'good',\n",
       " 'back']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "context_words = [i for i in pd.Series(corpus_bad_tokens_2).value_counts().head(n).index\n",
    "            if i in pd.Series(corpus_good_tokens_2).value_counts().head(n).index]\n",
    "context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65fd8800-2524-4096-8160-5d5debf3f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bad_tokens_3 = preprocess_text(corpus_bad,\n",
    "                            stop_words=stop_words,\n",
    "                            rare_words=less_5_words,\n",
    "                            context=context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c1c3d1c-5c1f-4819-831d-9f2b493adba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_good_tokens_3 = preprocess_text(corpus_good,\n",
    "                             stop_words=stop_words,\n",
    "                             rare_words=less_5_words,\n",
    "                             context=context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63aef1db-d7c1-477d-a10f-e909aa87eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_tokens_3 = preprocess_text(corpus_sample,\n",
    "                               stop_words=stop_words,\n",
    "                               rare_words=less_5_words,\n",
    "                               context=context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8be75-c91b-49b4-8206-e67c97d67334",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe8239-8983-4236-a9b0-9877562ee580",
   "metadata": {},
   "source": [
    "Les traitements étant longs, on sauvegarde les corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73db2ade-0818-480d-9f44-428890414e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = '../data/2_preprocessed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e05201b-8deb-404c-a24b-f177d38104ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH + \"corpus_bad_tokens\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_bad_tokens_2, fp)\n",
    "    \n",
    "with open(RESULTS_PATH + \"corpus_good_tokens\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_good_tokens_2, fp)\n",
    "    \n",
    "with open(RESULTS_PATH + \"corpus_sample_tokens\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_sample_tokens_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21a7e821-13c4-4ec3-b04a-86ea2bbf6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH + \"corpus_bad_without_context\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_bad_tokens_3, fp)\n",
    "    \n",
    "with open(RESULTS_PATH + \"corpus_good_without_context\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_good_tokens_3, fp)\n",
    "    \n",
    "with open(RESULTS_PATH + \"corpus_sample_without_context\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_sample_tokens_3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2649607-77a5-4603-906d-744b88cf2335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
